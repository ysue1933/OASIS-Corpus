Comprehensibility has emerged as a useful and intuitive means of globally evaluating second language (L2) speakers in many research and instructional contexts. In most cases, L2 speakersâ€™ comprehensibility is assessed by external listeners who do not engage in extensive communication with the speakers, even though the degree to which a speaker is comprehensible is presumably of greatest concern to their interlocutor. If comprehensibility is defined as the ease with which speakers come to understand one another, then interaction-based assessments, which would include self and peer ratings, might provide different insight into interactive comprehensibility compared to assessments by external listeners. To examine this issue, in this study, 20 pairs of L2 English interactants rated themselves and their partner on 7 occasions distributed throughout a 17-minute interaction encompassing 3 communicative tasks, and recordings of the interaction were subsequently presented to external raters for evaluation. Mixed-effects models were used to compare the shape of the comprehensibility curves over time and the self, partner, and rater scores at each rating episode. Results demonstrated that self and partner assessments were always aligned, but raters consistently assigned significantly lower comprehensibility scores to the interactants. These findings have implications for how comprehensibility, and indeed other listener-based constructs, are assessed.